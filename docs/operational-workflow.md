# Operational Workflow: What Gets Built, How It Runs, Who Does What

**Purpose:** Complete operational map of the Multi-Domain Knowledge Graph system
**Author:** Tyler Murray
**Date:** February 17, 2026
**Companion docs:** Knowledge Graph Architecture v3, Awake Integration Architecture

---

## System Overview

The knowledge graph system has seven major workflows. Each one needs a clear answer to three questions: What runs automatically? What needs Tyler? What tool does the work?

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    THE SEVEN WORKFLOWS                   â”‚
â”‚                                                          â”‚
â”‚  1. FEED MANAGEMENT        Who covers what                â”‚
â”‚  2. CONTENT ACQUISITION    Getting articles daily         â”‚
â”‚  3. ARTICLE PROCESSING     Summarize, embed, classify    â”‚
â”‚  4. GRAPH TRIAGE           Match articles to knowledge   â”‚
â”‚  5. DEEP RESEARCH          Build new knowledge           â”‚
â”‚  6. RETRIEVAL              Answer questions from graph   â”‚
â”‚  7. SCHEMA EVOLUTION       Grow the system over time     â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”‚
â”‚  â”‚ TOOLS                                          â”‚      â”‚
â”‚  â”‚                                                â”‚      â”‚
â”‚  â”‚ Claude Code    â†’ Builds all the software       â”‚      â”‚
â”‚  â”‚ Mac Mini M4    â†’ Runs everything 24/7          â”‚      â”‚
â”‚  â”‚ OpenClaw       â†’ Tyler's control interface     â”‚      â”‚
â”‚  â”‚ Neo4j AuraDB   â†’ The knowledge graph (cloud)   â”‚      â”‚
â”‚  â”‚ PostgreSQL     â†’ Article storage (local)       â”‚      â”‚
â”‚  â”‚ APIs           â†’ The brains (Gemini, Claude,   â”‚      â”‚
â”‚  â”‚                   Perplexity, OpenAI)           â”‚      â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Role Definitions

### Claude Code â€” The Builder

Claude Code builds every piece of software in this system. It writes the Python applications, the database schemas, the Cypher queries, the prompt templates, the cron configurations, and the monitoring scripts. Once built, these run on the Mac Mini without Claude Code's involvement.

Claude Code is also the iteration tool. When a synthesis prompt needs tuning, when triage thresholds need adjusting, when a new MasterDomain needs its pipeline configured â€” Tyler works with Claude Code to make changes, test them, and deploy.

**Claude Code does NOT run in production.** It builds things that run in production. Think of it as the engineer. The Mac Mini is the factory floor.

### Mac Mini M4 â€” The Factory Floor

The Mac Mini runs 24/7 in Tyler's home/office. It hosts:

- PostgreSQL database (Awake article storage)
- Python applications (all seven workflows)
- Cron scheduler (triggers workflows on schedule)
- OpenClaw (Tyler's interface to the system)
- Local monitoring and logging

It connects to:

- Neo4j AuraDB (cloud, via Bolt protocol)
- External APIs (Perplexity, OpenAI, Gemini, Claude â€” via HTTPS)

The Mac Mini is the single point of infrastructure. If it goes down, article fetching pauses and research queues build up, but nothing is lost â€” AuraDB is in the cloud, and the pipeline resumes where it left off when the Mac Mini comes back.

### OpenClaw â€” Tyler's Control Panel

OpenClaw is NOT an orchestrator in the traditional sense (it doesn't manage cron jobs or restart services â€” that's what systemd and cron do). OpenClaw is Tyler's **conversational interface** to the system. It's how Tyler interacts with the knowledge graph pipeline without opening a terminal.

**What OpenClaw does:**

- **Morning briefing delivery.** Every morning, OpenClaw sends Tyler a summary via WhatsApp/Telegram: articles processed overnight, gaps detected, research completed, anything needing review.
- **On-demand commands.** Tyler texts "what gaps were found today?" or "prioritize research on agentic checkout" or "show me coverage health for eCom" and OpenClaw translates that into database queries and pipeline commands.
- **Review interface.** When the pipeline flags something for Tyler's review (medium-confidence gaps, domain evolution signals, potential new MasterDomains), OpenClaw presents it conversationally and captures Tyler's decision (approve, reject, modify).
- **Ad-hoc research triggers.** Tyler texts "go deep on Kroger's precision marketing platform" and OpenClaw creates a ResearchPrompt, queues it at high priority, and notifies Tyler when results are ready.
- **System health.** Tyler asks "is everything running?" and OpenClaw checks cron logs, API status, database connections, and reports back.

**What OpenClaw does NOT do:**

- Run the pipelines directly (cron and Python do that)
- Make autonomous decisions about schema evolution
- Replace Claude Code for building/modifying software
- Serve as the primary data store for anything

**OpenClaw's technical role:** A long-running process on the Mac Mini that connects to WhatsApp/Telegram via API, monitors a set of database tables and log files, and can execute predefined commands (Python scripts) in response to Tyler's messages. It's a thin interface layer, not a brain.

### Tyler â€” The Strategist

Tyler's time investment: **15â€“30 minutes daily, 1 hour weekly.**

Daily (via OpenClaw, usually on phone):

- Review morning briefing (2 minutes)
- Approve/reject medium-confidence research prompts (5 minutes)
- Optionally direct ad-hoc research (as needed)

Weekly (via OpenClaw or direct dashboard):

- Review coverage health metrics across all domains (10 minutes)
- Review `:Unclassified` nodes and domain evolution signals (15 minutes)
- Approve/reject structural changes (new Domains, new NodeTypes) (10 minutes)
- Review feed health and signal-to-noise scores (5 minutes)
- Direct strategic priorities for the coming week (10 minutes)

Monthly:

- Synthesis prompt quality audit (review 20 random nodes for accuracy)
- Feed curation pass (add/remove feeds based on signal scores)
- Coverage depth assessment (are we deep enough in priority domains?)

### Neo4j AuraDB â€” The Knowledge Graph

Managed cloud service. Tyler doesn't administer it. The Python applications read and write to it via the Bolt protocol. It stores:

- All graph nodes (MasterDomains, Domains, KnowledgeNodes, Evidence, Context, Concepts, Articles, CoverageTopics, ResearchPrompts)
- Schema registry (NodeTypes, RelationshipCategories, SynthesisPrompts)
- Vector indexes for similarity search
- All relationships

### PostgreSQL â€” The Article Warehouse

Local database on the Mac Mini. Stores everything the graph doesn't need:

- Full article text (bulk storage)
- Raw HTML
- Processing metadata
- Triage results
- Feed configurations
- Signal-to-noise scores

---

## The Seven Workflows in Detail

---

### Workflow 1: Feed Management

**Purpose:** Maintain the right RSS feeds covering every MasterDomain.

**What's automated:**

- Circuit breaker monitoring (track fetch failures, flag dead feeds)
- Signal-to-noise scoring (monthly compute: what % of a feed's articles became Evidence?)
- Feed health report generation (feeds below 10% signal, feeds with 5+ consecutive failures)

**What's manual (Tyler):**

- Deciding which feeds to add for a new MasterDomain
- Removing underperforming feeds
- Promoting/demoting feed tiers based on signal scores
- Finding new feeds when coverage gaps point to missing sources

**What Claude Code builds:**

```
Application: feed_manager.py

Components:
â”œâ”€â”€ Feed CRUD (add, remove, update tier, activate/deactivate)
â”œâ”€â”€ Feed health monitor
â”‚   â”œâ”€â”€ Track consecutive fetch failures
â”‚   â”œâ”€â”€ Auto-deactivate after 30 days of failure
â”‚   â””â”€â”€ Generate weekly feed health report
â”œâ”€â”€ Signal-to-noise calculator
â”‚   â”œâ”€â”€ Monthly: count articles per feed
â”‚   â”œâ”€â”€ Count articles that became EVIDENCE_UPDATE
â”‚   â”œâ”€â”€ Compute signal ratio
â”‚   â””â”€â”€ Flag feeds below threshold
â””â”€â”€ Feed discovery helper
    â”œâ”€â”€ Given a list of topics, suggest RSS feed URLs
    â”œâ”€â”€ Validate RSS/Atom feed URLs
    â””â”€â”€ Test extraction quality (full text vs excerpt)

Database tables: feeds (PostgreSQL)
Cron schedule:
  - Health check: daily at 11 PM
  - Signal-to-noise: 1st of month at midnight
```

**OpenClaw integration:**

- Tyler texts: "add feed https://example.com/rss for eCom tier 2"
- Tyler texts: "show me feed health"
- Tyler texts: "which feeds are underperforming?"
- OpenClaw calls feed_manager.py functions and reports back

---

### Workflow 2: Content Acquisition

**Purpose:** Fetch articles from all active feeds on schedule, extract full text, deduplicate.

**What's automated (100%):**

- Scheduled RSS fetching per tier
- Full article text extraction
- URL and title deduplication
- Failure handling and retry logic
- Writing raw articles to PostgreSQL

**What's manual:** Nothing in steady state. Tyler might manually trigger a fetch if investigating something time-sensitive.

**What Claude Code builds:**

```
Application: article_fetcher.py

Components:
â”œâ”€â”€ RSS/Atom parser
â”‚   â”œâ”€â”€ Fetch feed XML
â”‚   â”œâ”€â”€ Parse entries (title, URL, published date, content)
â”‚   â”œâ”€â”€ Handle both RSS 2.0 and Atom formats
â”‚   â””â”€â”€ Track last-fetched entry per feed (don't re-process)
â”œâ”€â”€ Full text extractor
â”‚   â”œâ”€â”€ For excerpt-only feeds: fetch article URL
â”‚   â”œâ”€â”€ Readability-style content extraction (strip nav, ads, etc.)
â”‚   â”œâ”€â”€ Respect robots.txt and rate limits (2s between same-domain)
â”‚   â”œâ”€â”€ Handle failures gracefully (fall back to excerpt)
â”‚   â””â”€â”€ Store extraction_status: full | partial | failed
â”œâ”€â”€ Deduplicator
â”‚   â”œâ”€â”€ URL normalization (strip UTM, www, trailing slash)
â”‚   â”œâ”€â”€ Title similarity check (Jaccard >0.85 = probable dupe)
â”‚   â””â”€â”€ Mark duplicates, link to primary article
â””â”€â”€ PostgreSQL writer
    â”œâ”€â”€ Insert into articles table
    â”œâ”€â”€ Set processing_status = 'pending'
    â””â”€â”€ Handle conflicts (article already exists)

Database tables: articles, feeds (PostgreSQL)
Cron schedule:
  - Tier 1 feeds: every 4 hours (2AM, 6AM, 10AM, 2PM, 6PM, 10PM)
  - Tier 2 feeds: every 8 hours (2AM, 10AM, 6PM)
  - Tier 3 feeds: daily at 2AM
```

**Why 2 AM start:** Pipeline runs overnight so results are ready for Tyler's morning review. Tier 1 feeds also run during the day for time-sensitive signals.

---

### Workflow 3: Article Processing

**Purpose:** Summarize articles, extract structured data, generate embeddings, route to domains.

**What's automated (100%):**

- AI summarization and concept extraction (Gemini Flash)
- Embedding generation (OpenAI)
- Domain routing (rules engine)
- Writing processed data to PostgreSQL

**What's manual:** Nothing. But Tyler may review/tune the extraction prompt if quality drifts.

**What Claude Code builds:**

```
Application: article_processor.py

Components:
â”œâ”€â”€ Stage 1: Summarization & Extraction
â”‚   â”œâ”€â”€ Read pending articles from PostgreSQL
â”‚   â”œâ”€â”€ Call Gemini Flash API with extraction prompt
â”‚   â”œâ”€â”€ Parse structured output (summary, concepts, entities,
â”‚   â”‚   data_points, temporal_markers, article_type)
â”‚   â”œâ”€â”€ Write to article_processing table
â”‚   â””â”€â”€ Batch processing (10â€“20 articles per batch for efficiency)
â”œâ”€â”€ Stage 2: Embedding Generation
â”‚   â”œâ”€â”€ Take summary text from Stage 1
â”‚   â”œâ”€â”€ Call OpenAI text-embedding-3-small API
â”‚   â”œâ”€â”€ Store 1536-dim vector in article_processing table
â”‚   â”œâ”€â”€ Batch: up to 100 embeddings per API call
â”œâ”€â”€ Stage 3: Domain Routing
â”‚   â”œâ”€â”€ Rule 1: Feed primary_domain â†’ primary route
â”‚   â”œâ”€â”€ Rule 2: Concept embedding similarity vs Domain descriptions
â”‚   â”œâ”€â”€ Rule 3: Named entity matching (hardcoded entity â†’ domain map)
â”‚   â”œâ”€â”€ Rule 4: Cross-domain detection (articles matching 2+ domains)
â”‚   â”œâ”€â”€ Rule 5: No match â†’ route to "_unclassified"
â”‚   â””â”€â”€ Write routing results to triage_results table
â””â”€â”€ Error handling
    â”œâ”€â”€ API failures â†’ retry with exponential backoff
    â”œâ”€â”€ Malformed responses â†’ log and skip, don't block pipeline
    â””â”€â”€ Rate limiting â†’ respect, queue remaining

Database tables: article_processing, triage_results (PostgreSQL)
Cron schedule: Runs 30 minutes after each fetch cycle
  - Primary run: 2:30 AM (processes overnight Tier 1/2/3 articles)
  - Supplemental runs: 6:30 AM, 10:30 AM, 2:30 PM, 6:30 PM, 10:30 PM
    (processes Tier 1 articles fetched during the day)

Config files:
â”œâ”€â”€ extraction_prompt.txt (the Gemini Flash prompt â€” versioned)
â”œâ”€â”€ domain_routing_rules.json (entityâ†’domain mappings)
â””â”€â”€ embedding_config.json (model name, dimensions)
```

---

### Workflow 4: Graph Triage

**Purpose:** Match processed articles against the knowledge graph. Classify as evidence, gap signal, or noise. Inject into Neo4j.

**What's automated:**

- Stage 4: Vector similarity triage (100% automated)
- Stage 5: LLM gap classification (100% automated for high-confidence; flags medium-confidence for Tyler)
- Neo4j `:Article` node creation and relationship attachment
- ResearchPrompt creation for high-confidence gaps
- Content-level deduplication (embedding similarity check)

**What's manual (Tyler via OpenClaw):**

- Approving/rejecting medium-confidence gap classifications (5â€“10/day)
- Reviewing "potential_new_domain" signals (rare)

**What Claude Code builds:**

```
Application: graph_triage.py

Components:
â”œâ”€â”€ Content deduplication
â”‚   â”œâ”€â”€ Compare new article embedding vs articles from last 7 days
â”‚   â”œâ”€â”€ Cosine similarity > 0.92 â†’ mark as duplicate
â”‚   â”œâ”€â”€ Link duplicate to primary article
â”‚   â””â”€â”€ Skip remaining triage for duplicates
â”œâ”€â”€ Stage 4: Vector Similarity Triage
â”‚   â”œâ”€â”€ Connect to Neo4j AuraDB
â”‚   â”œâ”€â”€ For each article Ã— routed domain:
â”‚   â”‚   â”œâ”€â”€ Vector search: top 5 nearest KnowledgeNodes
â”‚   â”‚   â”œâ”€â”€ Classify by threshold:
â”‚   â”‚   â”‚   > 0.85 â†’ EVIDENCE_UPDATE
â”‚   â”‚   â”‚   0.60â€“0.85 â†’ PARTIAL_MATCH â†’ Stage 5
â”‚   â”‚   â”‚   0.40â€“0.60 â†’ POTENTIAL_GAP â†’ Stage 5
â”‚   â”‚   â”‚   < 0.40 â†’ NOISE_OR_BLIND_SPOT
â”‚   â”‚   â””â”€â”€ Write classification to triage_results
â”‚   â””â”€â”€ EVIDENCE_UPDATE actions:
â”‚       â”œâ”€â”€ Create (:Article) node in Neo4j
â”‚       â”œâ”€â”€ Create (:Article)-[:EVIDENCES]->(:KnowledgeNode)
â”‚       â””â”€â”€ Update KnowledgeNode.freshness_date
â”œâ”€â”€ Stage 5: LLM Gap Classification
â”‚   â”œâ”€â”€ Collect PARTIAL_MATCH + POTENTIAL_GAP articles
â”‚   â”œâ”€â”€ For each, build context:
â”‚   â”‚   â”œâ”€â”€ Article summary + concepts
â”‚   â”‚   â”œâ”€â”€ Top 3 nearest KnowledgeNodes (title + definition)
â”‚   â”‚   â””â”€â”€ Domain's CoverageTopic list
â”‚   â”œâ”€â”€ Call Claude Sonnet API with classification prompt
â”‚   â”œâ”€â”€ Parse classification output
â”‚   â”œâ”€â”€ Route by classification:
â”‚   â”‚   â”œâ”€â”€ new_subtopic (conf > 0.8):
â”‚   â”‚   â”‚   â†’ Auto: Create CoverageTopic + ResearchPrompt
â”‚   â”‚   â”œâ”€â”€ new_subtopic (conf 0.5â€“0.8):
â”‚   â”‚   â”‚   â†’ Queue for Tyler's review via OpenClaw
â”‚   â”‚   â”œâ”€â”€ new_angle:
â”‚   â”‚   â”‚   â†’ Create (:Article)-[:RELATES_TO]->(:KnowledgeNode)
â”‚   â”‚   â”œâ”€â”€ new_evidence:
â”‚   â”‚   â”‚   â†’ Create (:Article)-[:EVIDENCES]->(:KnowledgeNode)
â”‚   â”‚   â”œâ”€â”€ noise:
â”‚   â”‚   â”‚   â†’ Mark in PostgreSQL, no Neo4j action
â”‚   â”‚   â””â”€â”€ potential_new_domain:
â”‚   â”‚       â†’ Queue for Tyler's weekly review
â”‚   â””â”€â”€ Write all results to triage_results table
â”œâ”€â”€ Cluster Detection (weekly)
â”‚   â”œâ”€â”€ Find all NOISE articles from last 7 days
â”‚   â”œâ”€â”€ Cluster by embedding similarity (>0.80 mutual)
â”‚   â”œâ”€â”€ Clusters of 3+ â†’ emerging topic signal
â”‚   â””â”€â”€ Create high-priority CoverageTopic + ResearchPrompt
â”œâ”€â”€ Stale Node Detection (weekly)
â”‚   â”œâ”€â”€ Find KnowledgeNodes with freshness_score < 0.5
â”‚   â”œâ”€â”€ That have 3+ new EVIDENCES articles in last 14 days
â”‚   â”œâ”€â”€ That don't already have a queued ResearchPrompt
â”‚   â””â”€â”€ Auto-create refresh ResearchPrompts
â””â”€â”€ Domain Boundary Detection (monthly)
    â”œâ”€â”€ Find articles routed to a MasterDomain
    â”‚   but with no Domain match > 0.65
    â”œâ”€â”€ Group by concepts, count over 30 days
    â”œâ”€â”€ 5+ articles with shared concepts â†’ domain evolution signal
    â””â”€â”€ Queue for Tyler's weekly review

Database: PostgreSQL (triage_results) + Neo4j (Article nodes, relationships)
Cron schedule:
  - Triage: runs 15 minutes after article_processor completes
  - Cluster detection: Sundays at 3 AM
  - Stale node detection: Sundays at 4 AM
  - Domain boundary detection: 1st of month at 3 AM

Config files:
â”œâ”€â”€ triage_thresholds.json (similarity cutoffs â€” tunable)
â”œâ”€â”€ gap_classification_prompt.txt (Claude Sonnet prompt â€” versioned)
â””â”€â”€ cluster_config.json (similarity threshold, min cluster size)
```

---

### Workflow 5: Deep Research

**Purpose:** Execute research prompts from the queue, synthesize results into KnowledgeNodes, ingest into Neo4j.

**What's automated:**

- Research prompt execution (Perplexity/OpenAI APIs)
- Synthesis (Claude turning research into graph entities)
- Neo4j ingestion (creating nodes, relationships, lineage)
- Quality gate (verify Core properties, flag failures as `:Unclassified`)

**What's manual (Tyler):**

- Approving medium-confidence ResearchPrompts before execution
- Reviewing synthesis quality (monthly audit of 20 random nodes)
- Tuning synthesis prompts when quality drifts
- Directing ad-hoc research ("go deep on X")

**What Claude Code builds:**

```
Application: research_pipeline.py

Components:
â”œâ”€â”€ Research Queue Manager
â”‚   â”œâ”€â”€ Read ResearchPrompts from Neo4j (status: "queued")
â”‚   â”œâ”€â”€ Sort by priority (descending)
â”‚   â”œâ”€â”€ Filter: only execute prompts that are either
â”‚   â”‚   auto-approved (high confidence) OR Tyler-approved
â”‚   â”œâ”€â”€ Rate limit: max 20 prompts/day steady state,
â”‚   â”‚   max 200/day during domain build
â”‚   â””â”€â”€ Track execution status
â”œâ”€â”€ Research Executor
â”‚   â”œâ”€â”€ Primary: Perplexity Sonar Deep Research API
â”‚   â”‚   â”œâ”€â”€ Send prompt_text as query
â”‚   â”‚   â”œâ”€â”€ Wait for completion (~3 minutes)
â”‚   â”‚   â”œâ”€â”€ Parse response
â”‚   â”‚   â””â”€â”€ Store raw output in PostgreSQL (research_outputs table)
â”‚   â”œâ”€â”€ Supplemental: OpenAI o4-mini (for prompts flagged
â”‚   â”‚   as needing actionable/implementation detail)
â”‚   â”œâ”€â”€ Fallback: If Perplexity fails, retry once, then
â”‚   â”‚   try OpenAI o4-mini, then mark as failed
â”‚   â””â”€â”€ Parallelization: up to 5 concurrent requests
â”œâ”€â”€ Synthesizer
â”‚   â”œâ”€â”€ Read active SynthesisPrompt for target MasterDomain
â”‚   â”œâ”€â”€ Read schema registry (NodeTypes, RelationshipCategories,
â”‚   â”‚   property conventions) from Neo4j
â”‚   â”œâ”€â”€ Call Claude Sonnet API with:
â”‚   â”‚   â”œâ”€â”€ The SynthesisPrompt template
â”‚   â”‚   â”œâ”€â”€ The raw research output
â”‚   â”‚   â”œâ”€â”€ Schema registry context (valid types, properties,
â”‚   â”‚   â”‚   relationship categories)
â”‚   â”‚   â””â”€â”€ Existing KnowledgeNode titles in the same Domain
â”‚   â”‚       (to avoid duplicates)
â”‚   â”œâ”€â”€ Parse structured output:
â”‚   â”‚   â”œâ”€â”€ KnowledgeNodes (title, summary, definition,
â”‚   â”‚   â”‚   deep_content, confidence, claim_type)
â”‚   â”‚   â”œâ”€â”€ Evidence nodes (citation, methodology, strength, year)
â”‚   â”‚   â”œâ”€â”€ Context nodes (conditions, temporal_range, geographic)
â”‚   â”‚   â”œâ”€â”€ Concept nodes (definition, parent_concepts)
â”‚   â”‚   â”œâ”€â”€ Relationships (category, properties)
â”‚   â”‚   â””â”€â”€ Classification labels
â”‚   â””â”€â”€ Return structured graph entities
â”œâ”€â”€ Graph Ingestor
â”‚   â”œâ”€â”€ Create KnowledgeNode in Neo4j with Core + Standard properties
â”‚   â”œâ”€â”€ Generate embedding for each KnowledgeNode
â”‚   â”œâ”€â”€ Create Evidence, Context, Concept nodes
â”‚   â”œâ”€â”€ Create all relationships
â”‚   â”œâ”€â”€ Create lineage:
â”‚   â”‚   â”œâ”€â”€ (:KnowledgeNode)-[:CREATED_BY]->(:SynthesisPrompt)
â”‚   â”‚   â”œâ”€â”€ (:KnowledgeNode)-[:SOURCED_FROM]->(:ResearchOutput)
â”‚   â”‚   â””â”€â”€ (:KnowledgeNode)-[:BELONGS_TO]->(:Domain)
â”‚   â”œâ”€â”€ Update CoverageTopic status: "gap" â†’ "covered"
â”‚   â”œâ”€â”€ Update ResearchPrompt status: "queued" â†’ "completed"
â”‚   â””â”€â”€ Apply classification labels
â”œâ”€â”€ Quality Gate
â”‚   â”œâ”€â”€ Check: all Core properties present?
â”‚   â”œâ”€â”€ Check: at least 1 Evidence node attached?
â”‚   â”œâ”€â”€ Check: confidence is a valid float 0â€“1?
â”‚   â”œâ”€â”€ Check: relationships reference valid categories?
â”‚   â”œâ”€â”€ Pass â†’ node enters graph normally
â”‚   â””â”€â”€ Fail â†’ create (:Unclassified) node with raw content,
â”‚       failure reason, and source reference
â””â”€â”€ Bootstrap Mode
    â”œâ”€â”€ For new MasterDomain builds
    â”œâ”€â”€ Runs Passes 1â€“3 (landscape discovery, depth calibration,
    â”‚   external validation)
    â”œâ”€â”€ Creates CoverageTopic taxonomy
    â”œâ”€â”€ Executes deep research at higher throughput (50â€“100/day)
    â””â”€â”€ Includes cross-domain relationship identification

Database: PostgreSQL (research_outputs) + Neo4j (all graph entities)
Cron schedule:
  - Queue check: every 30 minutes, 6 AMâ€“10 PM
  - Execution: continuous when queue has approved items
  - Bootstrap mode: manual trigger via OpenClaw

Config files:
â”œâ”€â”€ synthesis_prompts/ (directory, one per MasterDomain + _default)
â”‚   â”œâ”€â”€ ecom_v1.txt
â”‚   â”œâ”€â”€ hyperscaler_v1.txt
â”‚   â””â”€â”€ _default_v1.txt
â”œâ”€â”€ research_config.json (API keys, rate limits, parallelization)
â””â”€â”€ quality_gate_rules.json (what counts as pass/fail)
```

**OpenClaw integration:**

- Tyler texts: "go deep on Kroger precision marketing" â†’ OpenClaw creates a high-priority ResearchPrompt
- Tyler texts: "what's in the research queue?" â†’ OpenClaw queries Neo4j for pending prompts
- Tyler texts: "approve all pending research" â†’ OpenClaw marks all medium-confidence prompts as approved
- Tyler texts: "run bootstrap for hyperscaler domain" â†’ OpenClaw triggers bootstrap mode
- Tyler texts: "pause research" / "resume research" â†’ OpenClaw enables/disables the research cron

---

### Workflow 6: Retrieval

**Purpose:** Answer Tyler's questions using the knowledge graph + Claude's extended thinking.

**What's automated:**

- Vector search + graph traversal (VectorCypherRetriever)
- Context serialization (reading templates from schema registry)
- Claude API call with retrieved context

**What's manual (Tyler):**

- Asking questions (this IS the product)

**What Claude Code builds:**

```
Application: retriever.py

Components:
â”œâ”€â”€ Query Handler
â”‚   â”œâ”€â”€ Accept natural language question
â”‚   â”œâ”€â”€ Generate embedding for the question
â”‚   â”œâ”€â”€ Optional: domain hint (if Tyler specifies "in eCom...")
â”‚   â””â”€â”€ Optional: depth hint ("give me the full picture" vs "quick answer")
â”œâ”€â”€ VectorCypherRetriever
â”‚   â”œâ”€â”€ Vector search: top 10 nearest KnowledgeNodes
â”‚   â”œâ”€â”€ Read RelationshipCategories from schema registry
â”‚   â”œâ”€â”€ Traverse all registered relationship categories within 2 hops
â”‚   â”œâ”€â”€ Collect Evidence, Context, Concept nodes
â”‚   â”œâ”€â”€ Collect cross-domain relationships (the unique value)
â”‚   â”œâ”€â”€ Shortest path analysis between top results
â”‚   â”‚   (Report 6: shortest paths contain more valuable context
â”‚   â”‚    than direct neighbors)
â”‚   â”œâ”€â”€ Collect contradictions (EPISTEMIC stance: "contradicts")
â”‚   â”‚   (Report 6: must be pre-annotated, Claude detects them
â”‚   â”‚    at near-random rates on its own)
â”‚   â””â”€â”€ Return subgraph
â”œâ”€â”€ Context Serializer
â”‚   â”œâ”€â”€ Read serialization templates from schema registry
â”‚   â”œâ”€â”€ Render each KnowledgeNode using its template
â”‚   â”œâ”€â”€ BFS ordering with most relevant content LAST
â”‚   â”‚   (Report 6: exploits recency bias)
â”‚   â”œâ”€â”€ Target 8K tokens (Report 6: universally optimal
â”‚   â”‚   for comprehensiveness)
â”‚   â”œâ”€â”€ Include relationship context between nodes
â”‚   â””â”€â”€ Pre-annotate contradictions with âš  markers
â”œâ”€â”€ Claude Caller
â”‚   â”œâ”€â”€ Model: Claude Opus 4.5 with extended thinking enabled
â”‚   â”œâ”€â”€ System prompt: domain expertise framing
â”‚   â”œâ”€â”€ Context: serialized subgraph from above
â”‚   â”œâ”€â”€ Question: Tyler's original query
â”‚   â””â”€â”€ Return response
â””â”€â”€ Response Augmentation
    â”œâ”€â”€ Track which KnowledgeNodes were used in the response
    â”œâ”€â”€ If no relevant nodes found â†’ log as "question failure"
    â”‚   â†’ auto-create ResearchPrompt (gap detection layer 4)
    â””â”€â”€ If response quality is low â†’ suggest deeper research

Integration: This is NOT a cron job. This runs on-demand.
Access methods:
  1. OpenClaw: Tyler texts a question â†’ retriever runs â†’
     response sent back via WhatsApp/Telegram
  2. CLI: Tyler runs `python retriever.py "question"` in terminal
  3. Future: Web UI or Claude.ai MCP integration
```

**OpenClaw integration:**

- Tyler texts: "Why is our retail media ROAS declining despite increased spend?"
- OpenClaw routes to retriever.py
- Retriever searches graph, serializes context, calls Claude Opus
- OpenClaw sends response back to Tyler via WhatsApp/Telegram
- If the graph can't answer well â†’ OpenClaw adds: "I don't have deep knowledge on [X]. Want me to research it?"

---

### Workflow 7: Schema Evolution

**Purpose:** Grow the graph's structure as new patterns emerge.

**What's automated:**

- `:Unclassified` node accumulation (from synthesis quality gate)
- Pattern detection in unclassified nodes
- Domain boundary signals from triage
- Generating suggestions for Tyler

**What's manual (Tyler + Claude Code):**

- Reviewing `:Unclassified` patterns and deciding what they mean
- Deciding to add a new NodeType, Property, RelationshipCategory, or Domain
- Working with Claude Code to implement the change
- Validating the change with a test synthesis run

**What Claude Code builds:**

```
Application: schema_monitor.py

Components:
â”œâ”€â”€ Unclassified Analyzer (weekly)
â”‚   â”œâ”€â”€ Query Neo4j for all `:Unclassified` nodes (reviewed: false)
â”‚   â”œâ”€â”€ Cluster by reason_unclassified and concepts
â”‚   â”œâ”€â”€ For clusters of 5+:
â”‚   â”‚   â”œâ”€â”€ Generate suggestion: "Consider adding NodeType: [X]"
â”‚   â”‚   â”œâ”€â”€ Include example content from the cluster
â”‚   â”‚   â””â”€â”€ Draft what the NodeType registry entry would look like
â”‚   â”œâ”€â”€ For one-offs: mark as reviewed, no action needed
â”‚   â””â”€â”€ Report to Tyler via OpenClaw
â”œâ”€â”€ Coverage Health Dashboard (weekly)
â”‚   â”œâ”€â”€ Coverage Score: % of CoverageTopics with status "covered"
â”‚   â”œâ”€â”€ Depth Score: avg Evidence nodes per KnowledgeNode
â”‚   â”œâ”€â”€ Within-Domain Connectivity: avg relationships within domain
â”‚   â”œâ”€â”€ Cross-Domain Connectivity: avg relationships spanning domains
â”‚   â”œâ”€â”€ Freshness Score: % of nodes above freshness threshold
â”‚   â”œâ”€â”€ Contestation Coverage: % with EPISTEMIC contradicts
â”‚   â”œâ”€â”€ Per-MasterDomain breakdown
â”‚   â””â”€â”€ Trend over time (compare to last week/month)
â”œâ”€â”€ Freshness Computation (weekly)
â”‚   â”œâ”€â”€ For each KnowledgeNode:
â”‚   â”‚   â”œâ”€â”€ Compute time since freshness_date
â”‚   â”‚   â”œâ”€â”€ Apply freshness_half_life decay function
â”‚   â”‚   â”œâ”€â”€ Set freshness_score (0â€“1)
â”‚   â”‚   â””â”€â”€ If below threshold â†’ check for existing ResearchPrompt
â”‚   â”‚       â†’ if none, create one
â”‚   â””â”€â”€ Report freshness summary to dashboard
â””â”€â”€ Schema Change Helper
    â”œâ”€â”€ NOT automated â€” Tyler runs manually via Claude Code
    â”œâ”€â”€ Adding a NodeType:
    â”‚   â”œâ”€â”€ Create (:NodeType) in registry
    â”‚   â”œâ”€â”€ Create serialization template
    â”‚   â”œâ”€â”€ Optionally reclassify :Unclassified nodes
    â”‚   â””â”€â”€ Update SynthesisPrompt to produce new type
    â”œâ”€â”€ Adding a Property:
    â”‚   â”œâ”€â”€ Update (:NodeType) in registry
    â”‚   â”œâ”€â”€ Update serialization template with COALESCE
    â”‚   â””â”€â”€ Optionally backfill existing nodes
    â”œâ”€â”€ Adding a RelationshipCategory:
    â”‚   â”œâ”€â”€ Create (:RelationshipCategory) in registry
    â”‚   â””â”€â”€ Update SynthesisPrompt
    â””â”€â”€ Adding a MasterDomain:
        â”œâ”€â”€ Create MasterDomain + Domain nodes
        â”œâ”€â”€ Create/fork SynthesisPrompt
        â”œâ”€â”€ Configure feeds (Workflow 1)
        â””â”€â”€ Trigger bootstrap (Workflow 5)

Cron schedule:
  - Unclassified analysis: Sundays at 5 AM
  - Coverage health: Sundays at 6 AM
  - Freshness computation: Sundays at 7 AM

  Results posted to Tyler via OpenClaw by 8 AM Sunday
```

---

## OpenClaw: The Full Interface Specification

OpenClaw ties everything together as Tyler's natural language control layer. Here's exactly what it handles:

### Morning Briefing (Daily, 7 AM)

OpenClaw automatically sends Tyler a morning message:

```
Good morning. Here's your overnight intelligence update:

ğŸ“¥ 312 articles processed (287 full text, 25 partial)
ğŸ“ 218 attached as evidence to existing knowledge
ğŸ” 14 new gaps detected (8 auto-queued, 6 need your review)
ğŸ”„ 7 stale nodes queued for refresh
ğŸ“Š Research completed: 5 new KnowledgeNodes added overnight

âš  2 items need your attention:
1. "In-store retail media measurement" â€” new subtopic?
   (conf: 0.72, source: AdExchanger Tier 1) [Approve / Reject]
2. "AI-powered planogram optimization" â€” new subtopic?
   (conf: 0.65, source: Retail Dive Tier 2) [Approve / Reject]

ğŸ”‹ System health: All green
```

### Command Set

Tyler can text any of these (or natural language variations):

**Research commands:**

- "Research [topic]" â†’ Creates high-priority ResearchPrompt
- "Go deep on [topic]" â†’ Creates ResearchPrompt with depth: "deep"
- "What's in the research queue?" â†’ Lists pending ResearchPrompts
- "Approve all pending research" â†’ Marks all medium-conf prompts as approved
- "Approve #3" â†’ Approves specific queued item
- "Reject #5" â†’ Rejects specific queued item
- "Pause research" / "Resume research" â†’ Toggles research pipeline

**Query commands:**

- "Ask: [any question]" â†’ Routes to retriever.py, returns answer
- "What do we know about [topic]?" â†’ Retriever with topic focus
- "Compare [X] vs [Y]" â†’ Retriever with comparison framing

**Status commands:**

- "Coverage health" â†’ Coverage health dashboard summary
- "Coverage health for [domain]" â†’ Domain-specific metrics
- "Feed health" â†’ Feed monitoring report
- "What gaps were found this week?" â†’ Gap detection summary
- "Show unclassified nodes" â†’ Unclassified analysis summary
- "System status" â†’ Checks all services, reports health

**Domain management:**

- "Add domain [name] under [MasterDomain]" â†’ Creates Domain node
- "Bootstrap [MasterDomain]" â†’ Triggers bootstrap mode
- "Add feed [URL] for [domain] tier [N]" â†’ Adds RSS feed

**Review commands:**

- "Show me what needs review" â†’ All pending items across workflows
- "Weekly review" â†’ Full weekly review dashboard

### OpenClaw Technical Implementation

```
Application: openclaw_interface.py

Components:
â”œâ”€â”€ Messaging Integration
â”‚   â”œâ”€â”€ WhatsApp Business API or Telegram Bot API
â”‚   â”œâ”€â”€ Receive messages from Tyler
â”‚   â”œâ”€â”€ Send formatted responses back
â”‚   â”œâ”€â”€ Handle rich formatting (tables, code blocks where supported)
â”‚   â””â”€â”€ Queue outgoing messages (don't flood)
â”œâ”€â”€ Command Parser
â”‚   â”œâ”€â”€ NLU layer: map Tyler's natural language to commands
â”‚   â”œâ”€â”€ Can use a small LLM (Gemini Flash or Claude Haiku)
â”‚   â”‚   for intent classification if regex isn't enough
â”‚   â”œâ”€â”€ Extract parameters (topic, domain, priority, etc.)
â”‚   â””â”€â”€ Confirm destructive actions before executing
â”œâ”€â”€ Command Executor
â”‚   â”œâ”€â”€ Maps parsed commands to Python function calls
â”‚   â”œâ”€â”€ Each workflow exposes a set of callable functions:
â”‚   â”‚   â”œâ”€â”€ feed_manager.add_feed()
â”‚   â”‚   â”œâ”€â”€ research_pipeline.create_prompt()
â”‚   â”‚   â”œâ”€â”€ research_pipeline.approve_prompt()
â”‚   â”‚   â”œâ”€â”€ retriever.ask()
â”‚   â”‚   â”œâ”€â”€ schema_monitor.coverage_health()
â”‚   â”‚   â””â”€â”€ etc.
â”‚   â”œâ”€â”€ Handles async operations (research takes minutes)
â”‚   â””â”€â”€ Sends result back via messaging integration
â”œâ”€â”€ Scheduled Messages
â”‚   â”œâ”€â”€ Morning briefing: daily at 7 AM
â”‚   â”œâ”€â”€ Weekly review: Sunday at 9 AM
â”‚   â”œâ”€â”€ Alert on critical signals (potential_new_domain,
â”‚   â”‚   system failures) immediately
â”‚   â””â”€â”€ Configurable quiet hours (no messages 10 PMâ€“6 AM
â”‚       except critical alerts)
â””â”€â”€ State Management
    â”œâ”€â”€ Track conversation context (so Tyler can say
    â”‚   "approve that" referring to the last presented item)
    â”œâ”€â”€ Track pending review items
    â””â”€â”€ Simple SQLite state store (not PostgreSQL â€”
        this is OpenClaw's internal state only)

Runs as: Long-running daemon process on Mac Mini
  Started via systemd service
  Auto-restarts on crash
  Logs to /var/log/openclaw/
```

---

## Complete File/Application Map

Everything Claude Code builds, organized by where it lives on the Mac Mini:

```
/home/tyler/knowledge-graph/
â”‚
â”œâ”€â”€ apps/
â”‚   â”œâ”€â”€ article_fetcher.py        â† Workflow 2
â”‚   â”œâ”€â”€ article_processor.py      â† Workflow 3
â”‚   â”œâ”€â”€ graph_triage.py           â† Workflow 4
â”‚   â”œâ”€â”€ research_pipeline.py      â† Workflow 5
â”‚   â”œâ”€â”€ retriever.py              â† Workflow 6
â”‚   â”œâ”€â”€ schema_monitor.py         â† Workflow 7
â”‚   â”œâ”€â”€ feed_manager.py           â† Workflow 1
â”‚   â””â”€â”€ openclaw_interface.py     â† OpenClaw
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ feeds.json                â† Feed URLs, tiers, domain mappings
â”‚   â”œâ”€â”€ triage_thresholds.json    â† Similarity cutoffs (tunable)
â”‚   â”œâ”€â”€ research_config.json      â† API keys, rate limits
â”‚   â”œâ”€â”€ domain_routing_rules.json â† Entity â†’ domain mappings
â”‚   â”œâ”€â”€ embedding_config.json     â† Model name, dimensions
â”‚   â”œâ”€â”€ quality_gate_rules.json   â† Synthesis quality checks
â”‚   â”œâ”€â”€ cluster_config.json       â† Cluster detection params
â”‚   â””â”€â”€ openclaw_config.json      â† Messaging API keys, quiet hours
â”‚
â”œâ”€â”€ prompts/
â”‚   â”œâ”€â”€ extraction_prompt.txt     â† Gemini Flash article extraction
â”‚   â”œâ”€â”€ gap_classification.txt    â† Claude Sonnet triage Stage 5
â”‚   â”œâ”€â”€ synthesis/
â”‚   â”‚   â”œâ”€â”€ ecom_v1.txt           â† eCom synthesis prompt
â”‚   â”‚   â”œâ”€â”€ hyperscaler_v1.txt    â† Hyperscaler synthesis prompt
â”‚   â”‚   â””â”€â”€ _default_v1.txt       â† Fallback synthesis prompt
â”‚   â””â”€â”€ retrieval_system.txt      â† Claude Opus system prompt
â”‚
â”œâ”€â”€ db/
â”‚   â”œâ”€â”€ schema.sql                â† PostgreSQL schema
â”‚   â”œâ”€â”€ migrations/               â† Database migration scripts
â”‚   â””â”€â”€ seed_data.sql             â† Initial domain/feed data
â”‚
â”œâ”€â”€ cron/
â”‚   â””â”€â”€ crontab.txt               â† All cron schedules
â”‚
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ openclaw.service           â† systemd service file
â”‚   â””â”€â”€ knowledge-graph.service    â† Optional: umbrella service
â”‚
â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ fetcher.log
â”‚   â”œâ”€â”€ processor.log
â”‚   â”œâ”€â”€ triage.log
â”‚   â”œâ”€â”€ research.log
â”‚   â””â”€â”€ openclaw.log
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_fetcher.py
â”‚   â”œâ”€â”€ test_processor.py
â”‚   â”œâ”€â”€ test_triage.py
â”‚   â”œâ”€â”€ test_research.py
â”‚   â”œâ”€â”€ test_retriever.py
â”‚   â””â”€â”€ test_integration.py       â† End-to-end pipeline test
â”‚
â””â”€â”€ docs/
    â”œâ”€â”€ knowledge-graph-architecture-v3.md
    â”œâ”€â”€ awake-integration-architecture.md
    â””â”€â”€ operational-workflow.md     â† This document
```

---

## Cron Schedule (All Times Mac Mini Local)

```
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# ARTICLE ACQUISITION & PROCESSING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Tier 1 feeds: every 4 hours
0 2,6,10,14,18,22 * * *    python apps/article_fetcher.py --tier 1

# Tier 2 feeds: every 8 hours
0 2,10,18 * * *             python apps/article_fetcher.py --tier 2

# Tier 3 feeds: daily
0 2 * * *                   python apps/article_fetcher.py --tier 3

# Process articles: 30 min after each fetch
30 2,6,10,14,18,22 * * *    python apps/article_processor.py

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# GRAPH TRIAGE
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Triage: 15 min after processing
45 2,6,10,14,18,22 * * *    python apps/graph_triage.py --daily

# Cluster detection: weekly
0 3 * * 0                   python apps/graph_triage.py --clusters

# Stale node detection: weekly
0 4 * * 0                   python apps/graph_triage.py --stale

# Domain boundary detection: monthly
0 3 1 * *                   python apps/graph_triage.py --boundaries

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DEEP RESEARCH
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Research queue check: every 30 min during waking hours
*/30 6-22 * * *             python apps/research_pipeline.py --process-queue

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# MONITORING & REPORTING
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Feed health check: daily
0 23 * * *                  python apps/feed_manager.py --health-check

# Signal-to-noise scoring: monthly
0 0 1 * *                   python apps/feed_manager.py --signal-score

# Unclassified analysis: weekly
0 5 * * 0                   python apps/schema_monitor.py --unclassified

# Coverage health: weekly
0 6 * * 0                   python apps/schema_monitor.py --coverage

# Freshness computation: weekly
0 7 * * 0                   python apps/schema_monitor.py --freshness

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# OPENCLAW
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

# Morning briefing: daily (OpenClaw sends via messaging)
0 7 * * *                   python apps/openclaw_interface.py --morning-briefing

# Weekly review: Sundays (OpenClaw sends via messaging)
0 9 * * 0                   python apps/openclaw_interface.py --weekly-review

# OpenClaw daemon runs as systemd service, not cron
```

---

## Build Sequence: What to Build When

### Phase 1: Core Pipeline (Week 1â€“2 with Claude Code)

**Goal:** Articles flowing from RSS â†’ PostgreSQL â†’ Neo4j with basic triage.

Build in this order:

1. **PostgreSQL schema** (`db/schema.sql`) â€” articles, feeds, article_processing, triage_results tables
2. **article_fetcher.py** â€” RSS parsing, full text extraction, dedup, PostgreSQL writes
3. **article_processor.py** â€” Gemini Flash summarization, OpenAI embedding, domain routing
4. **graph_triage.py** (Stage 4 only) â€” Neo4j vector similarity, EVIDENCE_UPDATE attachment
5. **Cron setup** â€” fetcher + processor + triage on schedule

**Test:** Add 10 eCom feeds. Run pipeline. Verify: articles land in PostgreSQL with summaries, embeddings generated, basic triage classifies articles against existing KnowledgeNodes (which are being built in parallel by Workflow 5).

**Dependency:** Requires at least 20 KnowledgeNodes in Neo4j to triage against. Build research_pipeline.py synthesis validation in parallel.

### Phase 2: Intelligence Layer (Week 2â€“3 with Claude Code)

**Goal:** Gap detection working. System identifies what it doesn't know.

Build:

6. **graph_triage.py** (Stage 5) â€” Claude Sonnet gap classification, ResearchPrompt creation, CoverageTopic creation
7. **research_pipeline.py** â€” Research queue manager, Perplexity executor, Claude synthesizer, Neo4j ingestor, quality gate
8. **Synthesis prompt v1** for eCom domain â€” the critical prompt that turns research into nodes
9. **Quality gate** â€” validate synthesis output, route failures to `:Unclassified`

**Test:** Run 20 research queries through the full pipeline. Tyler reviews 20 resulting nodes. Tune synthesis prompt. This is the critical gate â€” don't proceed until synthesis quality is validated.

### Phase 3: Retrieval (Week 3 with Claude Code)

**Goal:** Tyler can ask questions and get knowledge-graph-backed answers.

Build:

10. **retriever.py** â€” VectorCypherRetriever, context serializer (reads templates from registry), Claude Opus caller
11. **Question failure tracking** â€” log unanswerable questions, auto-create ResearchPrompts

**Test:** Tyler asks 10 real questions he'd ask in client meetings. Evaluate response quality vs Claude-without-graph. This is the first moment the system proves its value.

### Phase 4: OpenClaw (Week 3â€“4 with Claude Code)

**Goal:** Tyler can interact with the system from his phone.

Build:

12. **openclaw_interface.py** â€” messaging integration, command parser, command executor, scheduled messages
13. **Morning briefing** template and generator
14. **Command routing** to all workflow functions

**Test:** Tyler texts "what's in the research queue?" and gets a real answer. Tyler texts "research Kroger precision marketing" and a ResearchPrompt appears. Tyler texts "ask: why is retail media ROAS declining?" and gets a graph-backed answer.

### Phase 5: Monitoring & Feedback (Week 4â€“5 with Claude Code)

**Goal:** System monitors itself and recommends improvements.

Build:

15. **schema_monitor.py** â€” unclassified analyzer, coverage health dashboard, freshness computation
16. **feed_manager.py** â€” signal-to-noise scoring, feed health monitoring
17. **Cluster detection** and **stale node detection** in graph_triage.py
18. **Weekly review** format and delivery via OpenClaw

**Test:** Run for 2 weeks. Review weekly dashboard. Verify gap signals make sense. Tune thresholds based on real data.

### Phase 6: Scale & Polish (Week 5â€“6)

**Goal:** System runs autonomously. Tyler's involvement drops to 15â€“30 min/day.

19. Expand to full 30â€“60 feeds for eCom domain
20. Tune all thresholds based on accumulated data
21. Add error alerting (API failures, pipeline stalls)
22. Add logging and monitoring
23. Document operational runbooks (what to do when things break)
24. Domain boundary detection (monthly job)

### Phase 7+: Domain Expansion (1â€“2 weeks per domain)

For each new MasterDomain:

1. Tyler defines the domain and its sub-areas
2. Claude Code: Create MasterDomain + Domain nodes
3. Claude Code: Fork/create synthesis prompt
4. Tyler: Curate 30â€“60 RSS feeds
5. Claude Code: Configure feeds in system
6. Run bootstrap Passes 1â€“3 via research_pipeline.py
7. Tyler validates 20 nodes from synthesis
8. Open pipeline for that domain
9. OpenClaw starts routing that domain's articles and gaps

---

## Daily Operations Timeline

```
2:00 AM  â”¤ article_fetcher: Fetch all tiers
2:30 AM  â”¤ article_processor: Summarize + embed + route
2:45 AM  â”¤ graph_triage: Match against graph, classify
3:00 AM  â”¤ graph_triage: Stage 5 gap classification (Claude)
3:15 AM  â”¤ Pipeline complete. Results in PostgreSQL + Neo4j.
         â”‚
6:00 AM  â”¤ article_fetcher: Tier 1 refresh
6:30 AM  â”¤ article_processor: Process new Tier 1 articles
6:45 AM  â”¤ graph_triage: Triage Tier 1 articles
         â”‚
7:00 AM  â”¤ openclaw: Send morning briefing to Tyler
7:05 AM  â”¤ Tyler reviews briefing on phone (2 min)
7:10 AM  â”¤ Tyler approves/rejects flagged items (5 min)
         â”‚
7:30 AM  â”¤ research_pipeline: Pick up approved queue items
         â”¤ (runs every 30 min through the day)
         â”‚
10:00 AM â”¤ article_fetcher: Tier 1 refresh
10:30 AM â”¤ Process + triage cycle
         â”‚
[Repeats for Tier 1 at 2 PM, 6 PM, 10 PM]
[Tier 2 also runs at 10 AM and 6 PM]
         â”‚
11:00 PM â”¤ feed_manager: Health check
         â”‚
Ongoing  â”¤ openclaw: Listening for Tyler's messages 24/7
         â”¤ Tyler can ask questions, direct research,
         â”¤ check status anytime via WhatsApp/Telegram
```

---

## Failure Modes & Recovery

| Failure                   | Impact                                                                            | Auto-Recovery                                                                              | Manual Fix                                       |
| ------------------------- | --------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------ | ------------------------------------------------ |
| Mac Mini power loss       | All pipelines pause                                                               | Articles queue on RSS sources. Pipeline resumes on restart, processing anything missed.    | Reboot Mac Mini.                                 |
| Neo4j AuraDB outage       | Triage + research + retrieval stop. Fetching + processing continue to PostgreSQL. | Pipeline retries Neo4j connection every 5 min. Processes backlog when connection restored. | Check AuraDB status page.                        |
| Gemini Flash API down     | Article processing pauses                                                         | Articles queue in PostgreSQL as "pending." Resume when API returns.                        | Switch to backup model (Claude Haiku) in config. |
| Claude Sonnet API down    | Stage 5 gap classification pauses. Stages 1â€“4 continue working.                   | Stage 4 results still attach evidence. Stage 5 backlog processes when API returns.         | None needed â€” degraded but functional.           |
| Perplexity API down       | Deep research pauses. Gaps queue.                                                 | ResearchPrompts stay "queued." Execute when API returns.                                   | None needed.                                     |
| OpenAI Embedding API down | New articles can't be embedded or triaged                                         | Articles queue as "pending" in PostgreSQL.                                                 | Switch to backup embedding model in config.      |
| PostgreSQL crash          | Everything stops â€” no article storage                                             | Depends on backup strategy. Use daily pg_dump backups.                                     | Restore from backup.                             |
| OpenClaw crash            | Tyler loses phone interface. Everything else continues.                           | systemd auto-restarts the service.                                                         | Check logs, restart manually if needed.          |

**Key principle:** The pipeline degrades gracefully. Most failures only affect one stage. Articles queue rather than being lost. Neo4j data is in the cloud and not at risk from local failures. The most critical local data (PostgreSQL) should have daily backups.

---

## Cost Summary

### Monthly Operating Costs (Steady State, 1 Domain)

| Component                                     | Cost               |
| --------------------------------------------- | ------------------ |
| Neo4j AuraDB Professional (4GB)               | $263               |
| Gemini Flash (article summarization, 500/day) | $90â€“120            |
| OpenAI Embeddings (article + node embeddings) | $5â€“15              |
| Claude Sonnet (gap classification, ~100/day)  | $3â€“10              |
| Claude Sonnet (synthesis, ~20/day)            | $15â€“30             |
| Claude Opus (retrieval, ~10 queries/day)      | $20â€“50             |
| Perplexity API (deep research, 10â€“20/day)     | $120â€“240           |
| OpenAI o4-mini (supplemental research)        | $30â€“60             |
| **Total**                                     | **$550â€“790/month** |

### Scaling to 10 Domains

| Component          | 1 Domain     | 3 Domains      | 5 Domains        | 10 Domains       |
| ------------------ | ------------ | -------------- | ---------------- | ---------------- |
| AuraDB             | $263         | $263           | $500             | $500             |
| Article processing | $100â€“150     | $150â€“250       | $200â€“350         | $300â€“500         |
| Gap classification | $3â€“10        | $8â€“25          | $15â€“40           | $25â€“70           |
| Deep research      | $150â€“300     | $250â€“500       | $400â€“700         | $600â€“1,000       |
| Retrieval          | $20â€“50       | $25â€“60         | $30â€“70           | $40â€“90           |
| **Total**          | **$550â€“790** | **$700â€“1,100** | **$1,150â€“1,660** | **$1,470â€“2,160** |

### One-Time Build Costs

| Phase                          | Hours (Claude Code) | API Spend             |
| ------------------------------ | ------------------- | --------------------- |
| Core pipeline (Workflows 1â€“4)  | 8â€“12 hours          | $50â€“100 (testing)     |
| Research pipeline (Workflow 5) | 6â€“8 hours           | $100â€“200 (validation) |
| Retrieval (Workflow 6)         | 4â€“6 hours           | $20â€“50 (testing)      |
| OpenClaw (Workflow 7)          | 6â€“8 hours           | Minimal               |
| Monitoring (Workflow 7)        | 4â€“6 hours           | Minimal               |
| First domain build (eCom)      | â€”                   | $500â€“700 (research)   |
| **Total**                      | **28â€“40 hours**     | **$670â€“1,050**        |

Each subsequent domain: 4â€“6 hours Claude Code + $310â€“600 research API spend.
